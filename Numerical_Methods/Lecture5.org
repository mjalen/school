#+title: MATH 4200 Lecture 5
#+author: Jalen Moore

* 1.2 Review

Round-off error can be reduced by:

- Avoid subtraction of near equal values. *(Cancelation Error)* 
  - Multiply by conjugate.
  - Factor.
  - Trig. identities
  - Log. properties.
  - Taylor series.
- Reduce the number of operations.
  - Find an equivalent formula.
- Nested arithmetic: (For example, $x^2 + x + 1 = (x + 1)\cdot  x + 1$).
- Add in ascending order to preserve sigfigs.
- Avoid division an inaccurate value by a small value. *(Chapter 6)* 

  \begin{align*}
    \frac{x\pm 0.01}{0.0001627} = 6156.2815 \pm 61.46.
  \end{align*} 

  This can magnify the error.

* Algorithms and Convergence (1.3)

** Algorithm

*** Definition

A procedure or sequence of well-defined steps to produce a result in a finite amount of time.

*** Example

$S=\sum_{i=1}^N i^2$ can be described by the algorithm pseudo-code as follows.

- Input: $N$.
- Output: $S$.
- Steps:
  1. Set $S=0$.
  2. For $i=1,2,\ldots, N$ do $S=S+i^2$.
  3. Output $S$.

*** Example (Newton's Method)

To find a root, start with an initial guess $x_0$. Using the tangent line at $x_0$, the root of the tangent line $x_1$ is our next approximation. This can be iterated as so:

\begin{align*}
  x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} \text{ for } n=0,1,2,3,\ldots
\end{align*}


** Convergence

*** Definition

If $\lim_{n\rightarrow \infty} x_n = x$, then the sequence $\{x_n\}$ converges to $x$.

*** Efficiency

If $|x_n - x| \rightarrow 0$ quickly.

*** Rate
**** Definition

Assume $\{\beta_n\}$ and $\{\alpha_n\}$ converge. If $K > 0$ with $|\alpha_n - \alpha | \leq K | \beta_n |$ for a large $n$, then we say that $\{\alpha_n\}$ converges to $\alpha$ with rate (or order) of convergence $O(\beta_n)$. We write:

\begin{align*}
  \alpha_n = \alpha + O(\beta_n).
\end{align*}

Typically, $\beta_n = 1/n^p$ and look for the largest $p$. $\beta_n = C^n$ for $C\in(0,1)$.

- $\beta_n = 1/ (n!)$ is possible as well, but not as typical.

**** Example

Consider 

\begin{align*}
  x_n &= \frac{n^3+3n}{n^3 + 1};\\
  y_n &= \frac{n^4 + 5n -2}{n^4 + 2n}.
\end{align*}

Which converges faster?

\begin{align*}
  |x_n - x| &= \left| \frac{n^3+3n}{n^3+1} - 1 \right|,\\
  &= \left| \frac{n^3+3n - n^3 - 1}{n^3+1} \right|,\\
  &= \frac{3n-1}{n^3+1}.
\end{align*}

So, $x_n = 1 + O(\frac{1}{n^2})$. In contrast:

\begin{align*}
  |y_n - y| &= \left| \frac{n^4 + 5n -2}{n^4 + 2n} - 1 \right|,\\
  &= \left| \frac{n^4 + 5n -2 - n^4 - 5n + 2}{n^4 + 2n} \right|,\\
  &= \frac{3n-2}{n^4 + 2n}.
\end{align*}

So, $y_n = 1 + O(\frac{1}{n^3})$. Therefore, $y_n \rightarrow 1$ is faster than $x_n\rightarrow 1$

**** Example

Consider

\begin{align*}
  x_n = \frac{n+3}{n+7} \rightarrow 1;\\
  y_n = \frac{2^n + 3}{2^n + 7} \rightarrow 1.
\end{align*}

Then,

\begin{align*}
  |x_n - x| &= \left| \frac{n+3}{n+7} - 1\right|,\\
  &= \frac{4}{n+7}.
\end{align*}

So $x_n = 1 + O(1/n)$. Furthermore,

\begin{align*}
  |y_n - y| &= \left| \frac{2^n + 3}{2^n + 7} - 1 \right|,\\
  &= \frac{4}{2^n + 7}.
\end{align*}

So $y_n = 1 + O(1/2^n)$. Thus, $y_n$ is faster.

**** Example

Consider 

$x_n = \frac{\sin{n}}{n^2+1} \rightarrow 0$

Then,

\begin{align*}
  | x_n - 0 | &= \frac{|\sin{n} | }{n^2+1} \leq \frac{1}{n^2} 
\end{align*}

So, $x_n = 0 + O(1/n^2)$.

**** Example

Consider

\begin{align*}
  x_n = \cos{\frac{1}{n}} \rightarrow 1.
\end{align*}

We can bound the absolute value as:

\begin{align*}
  \left| \cos{\frac{1}{n}} - 1\right| \leq \frac{k}{n^p}.
\end{align*}

Using Taylor series truncated to $n=2$:

\begin{align*}
  \cos{x} = 1 - \frac{\cos{c}}{2} x^2.
\end{align*}

Then,

\begin{align*}
  \left| - \frac{\cos{c}}{2} (1/n)^2 \right| \leq \frac{1}{2n^2}.
\end{align*}


So, $x_n = 1 + O(1/n^2)$.

**** Example

$x_n = \ln{(n+1)} - \ln{n} =\ln{(\frac{n+1}{n})} = \ln{(1 + 1/n)}$

Using Taylor series: $\ln{1+x} = \frac{x}{1+c}$. The result is $x_n = 0 + O(1/n)$.

*** Upper Bound Convergence to Zero 

**** Definition

Given $G(h)\rightarrow 0$ and $F(h)\rightarrow L$ as $h\rightarrow 0$, if there is a constant $k>0$ with

\begin{align*}
  |F(h) - L | \leq K |G(h)|.
\end{align*}
- For a small $h$.

Then $F(h) = L + O(G(h))$.

**** Example

Given the limit derivative definition, the derivative of $f(x)$ converges at a speed of

\begin{align*}
  \frac{f(x+h)-f(x)}{h} = f'(x) + O(h).
\end{align*}

- This is proven using the Taylor series of $f(x)$ and $f(x_0+h)$.

**** Example

Consider $F(h) = (e^h - 1)/h \rightarrow 1$ as $h\rightarrow 0$.

\begin{align*}
  \left| \frac{e^h -1}{h} - 1 \right| &= \left| \frac{e^h - 1 - h}{h} \right|,\\ 
  &= \frac{e^c}{2} h,\\
  &\leq \frac{e}{2} h.
\end{align*}

Therefore, $F(h) = 1 + O(h)$.
